---
layout: post
title: Face detection on iOS
date: '2016-11-08T00:42:00.000+01:00'
author: Tomasz Cielecki
tags:
- Xamarin.iOS
- iOS
- Face Detection
- Xamarin
- Camera
modified_time: '2016-11-08T00:42:10.911+01:00'
thumbnail: https://4.bp.blogspot.com/-FIBY7_6v7wM/WCEQPi4KAeI/AAAAAAAADxs/qbKL2aP-0NQrzjwylnVmgdvQrg6DqC3gwCLcB/s72-c/norot.jpg
blogger_id: tag:blogger.com,1999:blog-3433282516380174051.post-4993771122532860292
blogger_orig_url: http://blog.ostebaronen.dk/2016/11/face-detection-on-ios.html
---

I've been playing a bit with the Camera on iOS lately and released a photo gallery/camera library called <a href="https://github.com/BruelAndKjaer/Chafu">Chafu</a>. iOS provides a fairly easy API to do all sorts of stuff, such as reading bar codes, QR codes and some other types of machine readable codes. It also supports finding faces!<br /><br />So as a fun feature I decided to figure out how to detect a face and show it live in the preview on screen when taking a photo, or recording a video, then add it in Chafu.<br /><br /><blockquote class="tr_bq">Note: iOS 7 added functionality to AVFoundation to do all the above, so what I describe here is iOS 7 and up, keep that in mind if you intend to do this on earlier versions.</blockquote>In this blog post I assume you already know how to set up a <i>AVCaptureSession</i>, <i>AVVCaptureDeviceInput </i>and <i>AVCaptureVideoPreviewLayer</i> to preview what is coming from the the Camera.<br /><br /><h3>Setup face detection</h3>First we need to add <i>AVCaptureMetadataOutput</i> to our session, this class is what detects faces and requires <i>IAVCaptureMetadataOutputObjectsDelegate</i> to be implemented in the class, which is where we get a callback when faces are detected.<br /><br />Setting up a <i>AVCaptureMetadataOuput</i> is simple<br /><ol><li>Instantiate it an instance of it</li><li>Add it to the <i>AVCaptureSession</i></li><li>Find out if faces metadata is available</li><li>Setup callback if 3. is OK</li></ol><br /><div><script src="https://gist.github.com/Cheesebaron/1a28878a4de820ed2ed40b65bead7402.js?file=SetupFaceDetection.cs"></script></div><br /><div>Notice <i>this</i> in the callback? That is the implementation of <i>IAVCaptureMetadataOutputObjectsDelegate</i>. I decided to add it directly to my Camera class. The implementation looks somewhat like follows.</div><br /><div><script src="https://gist.github.com/Cheesebaron/1a28878a4de820ed2ed40b65bead7402.js?file=IAVCaptureMetadataOutputObjectsDelegate.cs"></script></div><br /><div>The exportis pretty important, as this is how we tell the iOS world that we have implemented the interface, it is how it gets hold of our code.<br /><br />Now hopefully. When you run this code and set a break point in the <i><span class="pl-en">DidOutputMetadataObjects</span></i><span class="pl-en"> method, it would get hit when a face is detected. The argument <i>metadataObjects</i> will contain <i>AVMetadataFaceObject</i>s, which contains all you need to visually show the detected faces.&nbsp;</span><br /><span class="pl-en"><br /></span><br /><h3><span class="pl-en">Display faces</span></h3><div>To display faces I like using iOS layers, which gives us the possibility to do rotations and other transformations pretty easily. I will show this later in this post how to utilize the <i>Roll</i> and <i>Yaw</i> from the <i>AVMetadataFaceObject</i> when drawing the visual indicator for a face.</div><br /><div>First we need to set up the <i>CALayer</i> we will add detected faces to. This is the easiest as it will be easier later on just to clear the sublayers when we don't want to show faces anymore.</div></div><br /><div><script src="https://gist.github.com/Cheesebaron/1a28878a4de820ed2ed40b65bead7402.js?file=Overlay.cs"></script></div><br /><div>Now we can add faces to that layer in our <i><span class="pl-en">DidOutputMetadataObjects</span></i> method. This is done in a couple of simple steps.<br /><br /><ol><li>Iterate the <i>metadataObjects</i> array, which we get as argument</li><li>Make sure these are indeed <i>AVMetadataFaceObject</i></li><li>Transform the object through <i>AVCaptureVideoPreviewLayer</i>'s <i>GetTransformedMetadataObject</i> to get the correct coordinates for the face</li><li>Create a new <i>CALayer</i> for the face with a border or the desired effect</li><li>Set Bounds of the <i>CALayer</i> to what we got from the transformation</li><li>Add it as sublayer in the <i>overlayLayer</i> we created earlier</li></ol>&nbsp;Lets start by defining a method for how we want this layer which will show the face to look like.<br /><br /></div><br /><div><script src="https://gist.github.com/Cheesebaron/1a28878a4de820ed2ed40b65bead7402.js?file=NewLayer.cs"></script></div><br /><div>Here I simply create a new <i>CALayer</i> and set the border color, width and corner radius. So what we will see are white squares with border width 2 and rounded corners. Simple! <br /><br />Now, lets add this layer to the <i>overlayLayer</i> so we can actually see something on the screen.</div><br /><div><script src="https://gist.github.com/Cheesebaron/1a28878a4de820ed2ed40b65bead7402.js?file=DidOutputMetadataObjects.cs"></script></div><br /><div>That is it! Now you should have some squares showing up for faces. One problem though. This might add a whole bunch of layers. So we need to remove the sublayers before adding the new ones. </div><br /><div><script src="https://gist.github.com/Cheesebaron/1a28878a4de820ed2ed40b65bead7402.js?file=RemoveFaces.cs"></script></div><br /><div>Call <i>RemoveFaces()</i> method before you iterate <i>metadataObjects</i> in <i>DidOutputMetadataObjects</i> and this should.<br />The observant reader, might notice that this seems inefficient. I won't cover this in this article. However, you can see <a href="https://github.com/BruelAndKjaer/Chafu/blob/773f6d68dc03cb14d6bf55b4ec97322a20002f39/Chafu/BaseCameraView.cs#L535">one approach to solve this in <i>BaseCameraView</i> in Chafu</a>, where I keep track of the <i>FaceId</i> from the <i>AVMetadataFaceObject</i> and simply adjust bounds for that face if it has moved.<br /><br /><h3>Adjusting for Yaw and Roll angles</h3>The <i>AVMetadataFaceObject</i> gives us a <i>RollAngle</i> for when you rotate your head around the Z axis. It also gives us a <i>YawAngle </i>for rotations around the Y axis.<br /><br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://i.stack.imgur.com/j4WTO.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" src="http://i.stack.imgur.com/j4WTO.png" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Picture from StackOverflow: http://stackoverflow.com/q/16401505/368379</td></tr></tbody></table>Supporting <i>RollAngle</i> is easy as it does not require that we rotate into the Z plane, but rather around it. However, rotations around the Y axis, will move the rectangle into the Z plane. Per default, <i>CALayer</i> is flat and has no idea of perspective. We can fix that! Back to where we create the <i>overlayLayer</i> we need to add a simple transformation which will gives us this perspective.</div><br /><div><script src="https://gist.github.com/Cheesebaron/1a28878a4de820ed2ed40b65bead7402.js?file=Overlay2.cs"></script></div><br /><div>What this does is to take the default transformation and add a distance to the 3D projection plane in terms of <i>1/z</i>. In other words we add depth. Apple does this in reverse. Hence,<i> -1/z</i> is used, where z is the distance, in this case I use 1000. The bigger the value, the bigger the distance. For a more detailed explanation you could start by reading about <a href="https://en.wikipedia.org/wiki/3D_projection#Perspective_projection">3D projection on Wikipedia</a>.<br /><br />Now we can do our rotations to the face <i>CALayer</i>s.<br /><br /><h4>Roll</h4>To make a Roll rotation we simply create a new <i>CATransform3D</i> using the static <i>MakeRotation</i> method, which allows us to rotate around any axis. As shown above roll rotations are around the Z axis. <i>CATransform3D</i> expects the angle in radians. Hence, we need to convert that first.</div><br /><div><script src="https://gist.github.com/Cheesebaron/1a28878a4de820ed2ed40b65bead7402.js?file=RollTransform.cs"></script></div><br /><div>Pretty simple. We will apply this to the face layer later. <br /><h4>Yaw</h4>The Yaw rotation is a bit more involved. We need to know the orientation of the device as the Y axis changes depending on the orientation, and we need to adjust the angle for that. This means that faces are always detected in the same orientation. However, our preview layer will change along the orientation.</div><br /><div><script src="https://gist.github.com/Cheesebaron/1a28878a4de820ed2ed40b65bead7402.js?file=OrientationTransform.cs"></script></div><br /><div>Now we need to make the Yaw transformation and combine with the orientation transformation. </div><br /><div><script src="https://gist.github.com/Cheesebaron/1a28878a4de820ed2ed40b65bead7402.js?file=YawTransform.cs"></script></div><br /><div>Finally, apply the two transformations to the face <i>CALayer</i>, in the <i>DidOutputMetadataObjects</i> method. </div><br /><div><script src="https://gist.github.com/Cheesebaron/1a28878a4de820ed2ed40b65bead7402.js?file=DidOutputMetadataObjects2.cs"></script></div><br /><div>Notice, I add a default transformation to the <i>faceLayer</i> and concatenate the roll and/or yaw transform according to availability. That is it. Now, you should have something like this. Screenshots are taken from Chafu, which demonstrate detection with no rotation, then with roll and then with yaw.<br /><br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="float: left; margin-right: 1em; text-align: left;"><tbody><tr><td style="text-align: center;"><a href="https://4.bp.blogspot.com/-FIBY7_6v7wM/WCEQPi4KAeI/AAAAAAAADxs/qbKL2aP-0NQrzjwylnVmgdvQrg6DqC3gwCLcB/s1600/norot.jpg" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" height="320" src="https://4.bp.blogspot.com/-FIBY7_6v7wM/WCEQPi4KAeI/AAAAAAAADxs/qbKL2aP-0NQrzjwylnVmgdvQrg6DqC3gwCLcB/s320/norot.jpg" width="180" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">No rotation</td></tr></tbody></table><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="float: left; margin-right: 1em; text-align: left;"><tbody><tr><td style="text-align: center;"><a href="https://3.bp.blogspot.com/-DL3MPJgN-lo/WCEQPtMy9pI/AAAAAAAADxw/AdyYJQNFw-EbnP9WVrHNORMQYPhMdJceQCLcB/s1600/roll.jpg" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" height="320" src="https://3.bp.blogspot.com/-DL3MPJgN-lo/WCEQPtMy9pI/AAAAAAAADxw/AdyYJQNFw-EbnP9WVrHNORMQYPhMdJceQCLcB/s320/roll.jpg" width="180" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Roll</td></tr></tbody></table><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="float: left; margin-right: 1em; text-align: left;"><tbody><tr><td style="text-align: center;"><a href="https://4.bp.blogspot.com/-RnYM3DPCKAI/WCEQPziOkiI/AAAAAAAADx0/ViRoZoqbL04tSwyJIB4nIul44jp6JJCdQCLcB/s1600/yaw.jpg" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" height="320" src="https://4.bp.blogspot.com/-RnYM3DPCKAI/WCEQPziOkiI/AAAAAAAADx0/ViRoZoqbL04tSwyJIB4nIul44jp6JJCdQCLcB/s320/yaw.jpg" width="180" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Yaw</td></tr></tbody></table><br /></div>